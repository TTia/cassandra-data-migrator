# ============================================================================
# Cassandra Data Migrator - PostgreSQL Target Detailed Configuration
# ============================================================================
# Complete configuration reference for all PostgreSQL target options

# ==========================================================================
# ORIGIN CASSANDRA CONNECTION
# ==========================================================================

# Cassandra contact points (comma-separated for multiple nodes)
spark.cdm.connect.origin.host=cassandra1.example.com,cassandra2.example.com

# Native transport port (default: 9042)
spark.cdm.connect.origin.port=9042

# Authentication credentials
spark.cdm.connect.origin.username=cassandra_admin
spark.cdm.connect.origin.password=secure_password

# Consistency level for reads (default: LOCAL_QUORUM)
spark.cdm.connect.origin.consistencyLevel=LOCAL_QUORUM

# Local datacenter name (required for Cassandra 4.x)
spark.cdm.connect.origin.localDC=datacenter1

# Source keyspace and table
spark.cdm.schema.origin.keyspaceTable=production_keyspace.events_table

# ==========================================================================
# TLS/SSL Configuration for Origin (Optional)
# ==========================================================================

# Enable TLS for origin connection
#spark.cdm.connect.origin.tls.enabled=true
#spark.cdm.connect.origin.tls.trustStore.path=/path/to/truststore.jks
#spark.cdm.connect.origin.tls.trustStore.password=truststore_password
#spark.cdm.connect.origin.tls.trustStore.type=JKS

# ==========================================================================
# TARGET POSTGRESQL CONNECTION
# ==========================================================================

# Target database type: "postgres" or "postgresql"
spark.cdm.connect.target.type=postgres

# JDBC URL with optional parameters
# Basic format: jdbc:postgresql://host:port/database
# With parameters: jdbc:postgresql://host:port/database?ssl=true&sslmode=require
spark.cdm.connect.target.postgres.url=jdbc:postgresql://postgres-primary.example.com:5432/migration_target?ssl=true

# Authentication
spark.cdm.connect.target.postgres.username=migration_user
spark.cdm.connect.target.postgres.password=secure_pg_password

# Target schema (default: "public")
# Maps from Cassandra keyspace if not specified
spark.cdm.connect.target.postgres.schema=public

# Target table name
# If not specified, uses the origin table name
spark.cdm.connect.target.postgres.table=events

# ==========================================================================
# CONNECTION POOL CONFIGURATION
# ==========================================================================

# Maximum pool size (adjust based on PostgreSQL max_connections)
# Rule of thumb: num_partitions * 2 <= pool_size <= max_connections * 0.8
spark.cdm.connect.target.postgres.pool.size=20

# Connection timeout in milliseconds
# Time to wait for a connection from the pool
spark.cdm.connect.target.postgres.pool.timeout=30000

# ==========================================================================
# WRITE CONFIGURATION
# ==========================================================================

# Batch size: number of records per JDBC batch
# Larger batches = better throughput, more memory
# Recommended: 500-2000 for most workloads
spark.cdm.connect.target.postgres.batchSize=1000

# Upsert mode determines conflict handling:
#   "insert" - Simple INSERT, fails on duplicate keys
#   "upsert" - INSERT ON CONFLICT DO UPDATE
spark.cdm.connect.target.postgres.upsertMode=upsert

# Columns for ON CONFLICT clause (comma-separated)
# If not specified, uses primary key columns from target table
# Example: spark.cdm.connect.target.postgres.onConflictColumns=id,partition_key
#spark.cdm.connect.target.postgres.onConflictColumns=

# Transaction size: number of records per COMMIT
# Larger transactions = better performance, longer rollback time
spark.cdm.connect.target.postgres.transactionSize=10000

# Transaction isolation level
# Options: READ_COMMITTED, REPEATABLE_READ, SERIALIZABLE
spark.cdm.connect.target.postgres.isolationLevel=READ_COMMITTED

# ==========================================================================
# TYPE MAPPING CONFIGURATION
# ==========================================================================

# Convert Cassandra MAP types to PostgreSQL JSONB
# If false, requires custom array column types
spark.cdm.connect.target.postgres.mapToJsonb=true

# Convert Cassandra UDT (User Defined Types) to PostgreSQL JSONB
# If false, requires manual type handling
spark.cdm.connect.target.postgres.udtToJsonb=true

# ==========================================================================
# PERFORMANCE TUNING
# ==========================================================================

# Rate limiting for origin reads (records per second)
# Set to -1 for unlimited
spark.cdm.perfops.ratelimit.origin=50000

# Rate limiting for target writes (records per second)
# Consider PostgreSQL connection limits
spark.cdm.perfops.ratelimit.target=100000

# Number of rows to fetch per page from Cassandra
spark.cdm.perfops.fetchSizeInRows=5000

# Spark batch size for partitions
spark.cdm.perfops.batchSize=10

# ==========================================================================
# VALIDATION CONFIGURATION (for diff/validate mode)
# ==========================================================================

# Auto-correct missing records in target
# When true, inserts missing records during validation
spark.cdm.autocorrect.missing=false

# Auto-correct mismatched records in target
# When true, updates mismatched records during validation
spark.cdm.autocorrect.mismatch=false

# ==========================================================================
# PARTITION CONFIGURATION
# ==========================================================================

# Minimum partition token (default: use full range)
#spark.cdm.filter.cassandra.partition.min=-9223372036854775808

# Maximum partition token (default: use full range)
#spark.cdm.filter.cassandra.partition.max=9223372036854775807

# Number of splits per Spark partition
spark.cdm.perfops.numParts=10000

# ==========================================================================
# FEATURE FLAGS
# ==========================================================================

# Enable progress tracking table in target Cassandra (if available)
#spark.cdm.trackRun=false
#spark.cdm.trackRun.keyspace=system_cdm
#spark.cdm.trackRun.table=cdm_runs

# ==========================================================================
# COLUMN TRANSFORMATIONS (Optional)
# ==========================================================================

# Constant columns to add to target (name:type:value format)
#spark.cdm.feature.constantColumns.names=migration_timestamp,source_cluster
#spark.cdm.feature.constantColumns.values=NOW(),production

# Column mapping from origin to target (if names differ)
#spark.cdm.schema.origin.column.names.to.target=old_name1:new_name1,old_name2:new_name2

# ==========================================================================
# LOGGING & MONITORING
# ==========================================================================

# Log level for CDM operations
#spark.cdm.log.level=INFO
